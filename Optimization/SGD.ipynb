{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Gradient Descent vs Stochastic Gradient Descent (SGD) </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient means the rate of inclination or declination of a slope.\n",
    "Descent means the instance of descending.\n",
    "The algorithm is iterative means that we need to get the results multiple times to get the most optimal result.\n",
    "The iterative quality of the gradient descent helps a under-fitted graph to make the graph fit optimally to the data.\n",
    "The Gradient descent has a parameter called learning rate.  \n",
    "    Initially the steps are bigger that means the learning rate is higher and as the point goes down the learning rate becomes more smaller by the shorter size of steps. Also,the Cost Function is decreasing or the cost is decreasing .\n",
    "    \n",
    "<img src='https://www.bogotobogo.com/python/scikit-learn/images/Batch-vs-Stochastic-Gradient-Descent/stochastic-vs-batch-gradient-descent.png' height=30% width=30% />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence Rate of Stochastic Gradient Method\n",
    "The convergence of stochastic gradient descent has been analyzed using the theories of convex minimization and of stochastic approximation. Briefly, when the learning rates {\\displaystyle \\eta } \\eta  decrease with an appropriate rate, and subject to relatively mild assumptions, stochastic gradient descent converges almost surely to a global minimum when the objective function is convex or pseudoconvex, and otherwise converges almost surely to a local minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need terminologies like epochs, batch size, iterations only when the data is too big which happens all the time in machine learning and we can’t pass all the data to the computer at once.\n",
    "**One Epoch is when an ENTIRE dataset is passed forward and backward through the neural network only ONCE.**\n",
    "\n",
    "**Stochastic gradient descent\n",
    "typically reaches convergence much faster than gradient descent since it updates weight more frequently.**\n",
    "\n",
    "## Epoch <br>\n",
    "<span> One Epoch is when an ENTIRE dataset is passed forward and backward through the neural network only ONCE.<br>Since one epoch is too big to feed to the computer at once we divide it in several smaller batches. </span>\n",
    "passing the entire dataset through a neural network is not enough. And we need to pass the full dataset multiple times to the same neural network.\n",
    "\n",
    "One epoch leads to underfitting of the curve in the graph\n",
    "<img src='https://miro.medium.com/max/1050/1*i_lp_hUFyUD_Sq4pLer28g.png' width=50% height= 50% />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gradient descent which computes the gradient using the whole dataset, because the SGD tries to find minimums or maximums by iteration from a single randomly picked training example, the error is typically noisier than in gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stochastic gradient descent, the data sample should be in a random order, and this is why we want to shuffle the training set for every epoch. <br>\n",
    "The cost function is:<br>\n",
    "$J(\\mathbf{w})=\\frac{1}{2} \\sum_{i=1}^{N}\\left(y^{(i)}-\\phi\\left(\\mathbf{w}^{T} \\mathbf{x}\\right)^{(i)}\\right)^{2}$ <br>\n",
    "where the $\\phi$ is an activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://www.bogotobogo.com/python/scikit-learn/images/Batch-vs-Stochastic-Gradient-Descent/ActivationFunction.png' width=50% height=50% />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the weights that minimize our cost function, we can use optimization algorithm called gradient descent:\n",
    "<img src='https://www.bogotobogo.com/python/scikit-learn/images/Batch-vs-Stochastic-Gradient-Descent/GradientDescentCostFunction.png' width=30% height=30% />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight change Δ w is defined as the negative gradient multiplied by the learning rate η <br>\n",
    "$\\Delta \\mathbf{w}=-\\eta \\nabla J=\\eta \\sum_{i=1}^{N}\\left(y^{(i)}-\\phi\\left(\\mathbf{w}^{T} \\mathbf{x}\\right)^{(i)}\\right) \\mathbf{x}^{(i)}$ <br> gradient descent, the gradient is calculated from the whole training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Mini-batch gradient descent </h2> <br>\n",
    "This is a mixture of both stochastic and batch gradient descent. The training set is divided into multiple groups called batches. Each batch has a number of training samples in it. At a time a single batch is passed through the network which computes the loss of every sample in the batch and uses their average to update the parameters of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need terminologies like epochs, batch size, iterations only when the data is too big which happens all the time in machine learning and we can’t pass all the data to the computer at once. So, to overcome this problem we need to divide the data into smaller sizes and give it to our computer one by one and update the weights of the neural networks at the end of every step to fit it to the data given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Batch Size </h2><br>\n",
    "Total number of training examples present in a single batch. We can’t pass the entire dataset into the neural net at once. So, you divide dataset into Number of Batches or sets or parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterations <br>\n",
    "Iterations is the number of batches needed to complete one epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s say we have 2000 training examples that we are going to use .<br>\n",
    "<b>Example :We can divide the dataset of 2000 examples into batches of 500 then it will take 4 iterations to complete 1 epoch.</b><br>\n",
    "Where Batch Size is 500 and Iterations is 4, for 1 complete epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
