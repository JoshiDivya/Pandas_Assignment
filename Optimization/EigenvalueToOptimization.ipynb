{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Gradient Operator?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The Gradient (also called the Hamilton operator) is a vector operator for any N-dimensional scalar function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(x_1,\\cdots, x_N)=f({\\bf x})$, where ${\\bf x}=[x_1,\\cdots,x_N]^T$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* is an N-D vector variable. For example, when \n",
    "$N=3$, $f({\\bf x})$ \n",
    "\n",
    " may represent temperature, concentration, or pressure in the 3-D space. The gradient of this N-D function is a vector composed of\n",
    "$N$ components for the $N$ partial derivatives: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"403\" height=\"58\" border=\"0\" src=\"img19.png\" alt=\"\\begin{displaymath}\n",
    "{\\bf g}({\\bf x})=\\bigtriangledown f({\\bf x})=\\frac{d}{d{\\bf ...\n",
    "...x_1},\\cdots,\n",
    "\\frac{\\partial f({\\bf x})}{\\partial x_N}\\right]^T\n",
    "\\end{displaymath}\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The direction $\\angle {\\bf g}$ of the gradient vector ${\\bf g}$ is the direction in the N-D space along which the function  $f({\\bf x})$ increases most rapidly.\n",
    "The magnitude $\\vert{\\bf g}\\vert$ of the gradient ${\\bf g}$ is the rate of the increment.\n",
    "In image processing we only consider 2-D field: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"93\" height=\"79\" border=\"0\" src=\"img23.png\" alt=\"\\begin{displaymath}\n",
    "\\bigtriangledown=\\left[\\begin{array}{c}\n",
    "\\frac{\\partial}{\\partial x}  \\frac{\\partial}{\\partial y} \\end{array}\\right]\n",
    "\\end{displaymath}\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hessian Matrix \n",
    "* It is a square matrix or second-order partial derivative of scalar valued function. \n",
    "$${\\displaystyle \\mathbf {H} ={\\begin{bmatrix}{\\dfrac {\\partial ^{2}f}{\\partial x_{1}^{2}}}&{\\dfrac {\\partial ^{2}f}{\\partial x_{1}\\,\\partial x_{2}}}&\\cdots &{\\dfrac {\\partial ^{2}f}{\\partial x_{1}\\,\\partial x_{n}}}\\\\[2.2ex]{\\dfrac {\\partial ^{2}f}{\\partial x_{2}\\,\\partial x_{1}}}&{\\dfrac {\\partial ^{2}f}{\\partial x_{2}^{2}}}&\\cdots &{\\dfrac {\\partial ^{2}f}{\\partial x_{2}\\,\\partial x_{n}}}\\\\[2.2ex]\\vdots &\\vdots &\\ddots &\\vdots \\\\[2.2ex]{\\dfrac {\\partial ^{2}f}{\\partial x_{n}\\,\\partial x_{1}}}&{\\dfrac {\\partial ^{2}f}{\\partial x_{n}\\,\\partial x_{2}}}&\\cdots &{\\dfrac {\\partial ^{2}f}{\\partial x_{n}^{2}}}\\end{bmatrix}}.}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "${\\displaystyle \\mathbf {H} _{i,j}={\\frac {\\partial ^{2}f}{\\partial x_{i}\\partial x_{j}}}.}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The determinant of the above matrix is also sometimes referred to as the Hessian.\n",
    "The Hessian matrix is related to the Jacobian matrix by H(f(x)) = J(∇f(x))T."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Gradient Definition__\n",
    "\n",
    "Let $f:Rn→Rm$.\n",
    "\n",
    "We say, that f is differentiable at pointx0, if there exists a linear transformation A(x0), such that $f(x0+∆x) =f(x0) +A(x0)∆x+o(∆x)$\n",
    "\n",
    "We call a function  f differentiable on a set $Q⊂Rn$, \n",
    "\n",
    "if it is differentiable at each point of Q. If  f  is differentiable on Rn, we just say that  f  is differentiable. \n",
    "\n",
    "\n",
    "The matrix A(x0)  is referred to as the  derivative  or  Jacobi matrix of   f(at pointx0),\n",
    "and is denoted by $∂f(x0)∂xorf′(x0)$.\n",
    "\n",
    "Exercise:What doeso(∆x) mean in the previous definition and why isn’t ito(‖∆x‖)?\n",
    "\n",
    "Theorem 1.1$(A(x0))ij=∂fi(x0)∂xj$\n",
    "\n",
    "Exercise:\n",
    "\n",
    " First consider $f:Rn→R$ and use the total differential formula $df(x) =∑ni=1∂f∂xidxi$\n",
    " \n",
    "Theorem 1.2\n",
    "\n",
    "$(f(g(x))′=f′(g(x))g′(x)$\n",
    "\n",
    "Exercise:\n",
    "\n",
    "First consider $f:Rn→R$ and use the chain rule $∂f◦g∂xj(x) =∑ni=1∂f∂xi(g(x))·∂gi∂xj(x)$.\n",
    "\n",
    "Let $f:Rn→R$ be differentiable. Its derivative (at pointx0) is then a $1×nmatrixA(x0) =(∂f(x0)∂x1∂f(x0)∂x2...∂f(x0)∂xn)$\n",
    "\n",
    "The vectorA(x0)Tis referred to as the gradient of  f and is denoted as grad  $xf(x0) or∇xf(x0)$\n",
    "or simply $∇f(x0)$.\n",
    "\n",
    "Theorem 1.3 \n",
    "\n",
    "$∇(f(x)g(x)) =∇f(x)g(x) +f(x)∇g(x)∇(f(x)/g(x)) = (∇f(x)g(x)−f(x)∇g(x))/g2(x)∇(f(g(x))) =∇f(g(x))∇g(x) =f′(g(x))∇g(x)$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "__Hessian Definition__\n",
    "\n",
    "Let $f:Rn→R$ be differentiable and $∇f:Rn→Rn$  differentiable (i.e. let $f$ be twice differentiable). \n",
    "\n",
    "The derivative of$∇f$ at point$x0$ is $ann×nmatrixH(x0)$\n",
    "\n",
    "which is referred to as the second derivativeor Hessian of $f$(at pointx0) and is denoted  as \n",
    "\n",
    "$∂2f(x0)∂2xor∇2f(x0)$.\n",
    "\n",
    "Theorem 2.1\n",
    "\n",
    "$(H(x0))ij=∂2f(x0)∂xi∂xj$\n",
    "\n",
    "Exercise: Statement follows from (1.1).\n",
    "\n",
    "\n",
    "Theorem 2.2\n",
    "\n",
    "If partial derivatives $∂2f∂xi∂xjand∂2f∂xj∂xi$  of function $f$ are continuous at $x0$, then they are equal.\n",
    "\n",
    "Thus, Hessian of a sufficiently smooth function is a symmetric matrix.\n",
    "\n",
    "Theorem 2.3\n",
    "\n",
    "All eigenvalues of a symmetric matrix are real.\n",
    "\n",
    "Theorem 2.4\n",
    "\n",
    "The set of eigenvectors of a symmetric matrix contains an orthonormalbasis as a subset.\n",
    "\n",
    "\n",
    "Theorem 2.5\n",
    "\n",
    "$f(x0+∆x) =f(x0) +∂f(x0)∂x∆x+12∆xT(∂2f(x0)∂2x)∆x+o(‖∆x‖2)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relation between quadratic form, Hessian, Precision matrix, Mahalanobis distance and Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "__Hessian and Optimization__\n",
    "\n",
    "A gradient descent with step size 𝛼\n",
    "\n",
    "can be expressed as:\n",
    "\n",
    "𝑥𝑡+1=𝑥𝑡−𝛼(𝐻𝑥𝑡−𝑏)\n",
    "\n",
    "Where 𝐻\n",
    "\n",
    "is the Hessian matrix\n",
    "\n",
    "In the direction of the 𝑖𝑡ℎ\n",
    "\n",
    "eigen vector,\n",
    "\n",
    "𝑥𝑡+1=(1−𝛼𝜆𝑖)𝑡𝑥0\n",
    "\n",
    "where 𝑥0\n",
    "\n",
    "is the initial vector\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Momentum__\n",
    "\n",
    "An object which is moving has momentum. The amount of momentum (p) possessed by the moving object is the product of mass (m) and velocity (v). In equation form:\n",
    "\n",
    "$$p = m • v$$\n",
    "\n",
    "9An equation such as the one above can be treated as a sort of recipe for problem-solving. Knowing the numerical values of all but one of the quantities in the equations allows one to calculate the final quantity in the equation. An equation can also be treated as a statement which describes qualitatively how one variable depends upon another. Two quantities in an equation could be thought of as being either directly proportional or inversely proportional. Momentum is directly proportional to both mass and velocity. A two-fold or three-fold increase in the mass (with the velocity held constant) will result in a two-fold or a three-fold increase in the amount of momentum possessed by the object. Similarly, a two-fold or three-fold increase in the velocity (with the mass held constant) will result in a two-fold or a three-fold increase in the amount of momentum possessed by the object. Thinking and reasoning proportionally about quantities allows you to predict how an alteration in one variable would effect another variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function and Cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Loss function** is usually a function defined on a data point, prediction and label, and measures the penalty. For example:\n",
    "    * square loss $ l(f(xi|θ),yi)=(f(xi|θ)−yi)^2 $,used in linear regression\n",
    "    * hinge loss $ l(f(xi|θ),yi)=max(0,1−f(xi|θ)yi) $used in SVM\n",
    "    * 0/1 loss $ l(f(xi|θ),yi)=1⟺f(xi|θ)≠yi $, used in theoretical analysis and definition of accuracy\n",
    "\n",
    "* **Cost function** is usually more general. It might be a sum of loss functions over your training set plus some model complexity penalty (regularization). For example:\n",
    "    * Mean Squared Error MSE(θ)=1N∑Ni=1(f(xi|θ)−yi)^2\n",
    "    * SVM cost function SVM(θ)=∥θ∥2+C∑Ni=1ξi (there are additional constraints connecting ξi with C and with training set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
